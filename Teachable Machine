import numpy as np
import tflite_runtime.interpreter as tflite
import cv2
from picamera2 import Picamera2

# Load Teachable Machine model
MODEL_PATH = "/home/user/Downloads/model2.tflite"
LABELS_PATH = "/home/user/Downloads/labels2.txt"

# Load labels
with open(LABELS_PATH, "r") as f:
    labels = [line.strip() for line in f.readlines()]

# Initialize TensorFlow Lite interpreter
interpreter = tflite.Interpreter(model_path=MODEL_PATH)
interpreter.allocate_tensors()

# Get input and output details
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Get model input shape
input_shape = input_details[0]['shape']  # [batch, height, width, channels]
height, width, channels = input_shape[1], input_shape[2], input_shape[3]
input_dtype = input_details[0]['dtype']

print(f"Model expects input shape: {input_shape}, Data type: {input_dtype}")

# Initialize Picamera2
picam2 = Picamera2()
config = picam2.create_still_configuration(main={"size": (640, 480)})  # Adjust resolution if needed
picam2.configure(config)
picam2.start()

# Function to preprocess image (convert to edge detection, resize, normalize)
def preprocess_image(image):
    # Convert to grayscale
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # Apply edge detection (Canny)
    edges = cv2.Canny(gray, 100, 200)

    # Convert back to 3-channel image if needed
    if channels == 3:
        edges = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)
    elif channels == 1:
        edges = np.expand_dims(edges, axis=-1)  # Keep single channel

    # Resize image to match model input size
    resized_frame = cv2.resize(edges, (width, height))

    # Expand dimensions to match model input (batch size 1)
    input_data = np.expand_dims(resized_frame, axis=0).astype(np.float32) / 255.0

    return input_data, edges

# Function to generate expected shape images
def generate_expected_shapes():
    expected_shapes = []
    for label in labels:
        img = np.zeros((height, width), dtype=np.uint8)  # Black background
        cv2.putText(img, label, (10, height // 2), cv2.FONT_HERSHEY_SIMPLEX, 0.8, 255, 2)
        expected_shapes.append((label, img))
    return expected_shapes

# Display expected shapes
expected_shapes = generate_expected_shapes()
for label, shape_img in expected_shapes:
    cv2.imshow(f"Expected Shape: {label}", shape_img)

print("Starting real-time detection...")

try:
    while True:
        # Capture image
        frame = cv2.cvtColor(picam2.capture_array(), cv2.COLOR_RGB2BGR)

        # Preprocess image
        input_data, edges = preprocess_image(frame)

        # Run inference
        interpreter.set_tensor(input_details[0]['index'], input_data)
        interpreter.invoke()
        output_data = interpreter.get_tensor(output_details[0]['index'])
        predicted_class = np.argmax(output_data)

        # Get predicted label
        prediction_label = labels[predicted_class]
        print(f"Detected: {prediction_label}")

        # Display results on screen
        cv2.putText(frame, f"Detected: {prediction_label}", (10, 50),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)

        # Show both original and edge-detected frames
        cv2.imshow("Teachable Machine Recognition", frame)
        cv2.imshow("Edge Detection", edges)

        # Exit on 'q' key press
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

except KeyboardInterrupt:
    print("Program stopped.")
